---
title: "Method (data processing)"
author: "Thomas FitzGerald, Mei Maddox, Kyle Rodriguez"
date: "4/13/2022"
output: html_document
---

To process our corpus, we used Spark, a distributed computing platform well-suited to handling large amounts of data.  This means that, although our test sample only used 20 stories total, our method should scale reasonably well if we were to run hundreds or thousands of stories.

Each book or fanfiction in our corpus was broken down into a list of sentences.  We used spark to run natural language processing
from the nltk package on each sentence, converting that sentence into a list of tokens.  Each token represents a word or punctuation mark, which, through some additional processing, becomes the data we produced.  

Nltk does a fairly good job of determining parts of speech, using context to guess POS for unknown terms.  There are a few cases that return odd results (a non-standard ellipse, like '....', often returns as a noun), but, on average, it's pretty accurate.

The data returns as a series of .csv's, one aggregated by literature type, and one broken down by individual story.  The second one would not scale well to thousands of pieces, but could be modified to look at variance within each literature type.